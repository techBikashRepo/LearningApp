# Request-Response Lifecycle — Part 3 of 3

### Topic: AWS SAA Exam Focus, Comparison Tables, Quick Revision

**Series:** Networking Fundamentals → System Design → AWS Solutions Architect
**Sections Covered:** 9, 10, 11, 12

---

## SECTION 9 — AWS SAA Certification Focus

### Exam-Relevant Services Per Lifecycle Phase

```
Phase 1 — DNS Resolution:
  Route 53: all DNS record types (A, AAAA, CNAME, Alias, MX, TXT, SRV)
  CRITICAL: Alias record vs CNAME
    CNAME: cannot point to root domain (apex): https://shop.com → CNAME → NOT ALLOWED
    Alias: CAN point to root domain → ALB, CloudFront, S3 website, API Gateway

  Route 53 Routing Policies:
    Simple: one IP
    Weighted: A/B testing, gradual migration (5% → new version)
    Latency-based: returns record for AWS region with lowest latency to client
    Geolocation: route European users to EU region (data residency compliance)
    Geoproximity: like geolocation but with configurable "bias" radius
    Failover: primary → secondary when health check fails
    Multi-value: returns up to 8 healthy IPs (NOT a load balancer — disclaimer on exam)

  Exam trap: "Lowest latency routing" = Latency-based (not geolocation)
  Exam trap: "Weighted with 0 weight" = traffic never reaches that target
  Exam trap: Multi-value answer routing is NOT a substitute for a load balancer

Phase 2 — TCP/HTTP Connection:
  ALB: Layer 7, HTTP/HTTPS, path-based routing, host-based routing, gRPC support
  NLB: Layer 4, TCP/UDP/TLS, static IP, ultra-low latency (<1ms)

  Exam trap: "Static IP for whitelist" → NLB (not ALB — ALB has dynamic IPs)
  Exam trap: "WebSocket" → ALB supports WebSocket natively (not NLB which is L4 passthrough)
  Exam trap: "Client IP preservation" →
    ALB: X-Forwarded-For header has client IP
    NLB: actual client IP preserved (no proxy headers needed)
  Exam trap: "TLS termination at load balancer" → ALB or NLB TLS listener
  Exam trap: "mTLS (mutual TLS) for API clients" → ALB mutual authentication in 2023+ OR API Gateway mTLS

Phase 3 — TLS:
  ACM: free TLS certs for ALB, CloudFront, API Gateway
  ACM certs: CANNOT be used on EC2 directly (only on AWS-managed load balancers + CloudFront)

  Exam trap: "TLS cert on EC2" → import cert to ACM or store in Secrets Manager / Parameter Store
  Exam trap: "Cert expiry automation" → ACM auto-renews for supported services; set CloudWatch alarm for expiry

Phase 4-6 — HTTP + Server Processing:
  Lambda: max 15-minute timeout; 6MB sync invocation payload limit; 256KB async
  API Gateway: max 29-second integration timeout (hard limit for REST and HTTP APIs)
  ALB: idle timeout max 4000 seconds; deregistration delay 0-3600s

  Exam trap: "Lambda processing > 29s called via API Gateway" → impossible! API GW times out
  Solution pattern: API GW → Lambda → SQS → background Lambda → webhook/polling for result

Phase 7 — HTTP Response:
  CloudFront: returns X-Cache: Hit or Miss header (check if content was cached)
  ALB: returns X-Forwarded-For, X-Forwarded-Proto, X-Forwarded-Port

Phase 8 — Rendering (browser):
  Not directly in SAA scope — but CloudFront S3 static hosting + SPA patterns are
  S3 static website + CloudFront = global SPA delivery
```

### Critical Metric Pairs (Commonly Confused on Exam)

```
ALB Metrics — know these exactly:
  RequestCount: total requests received by ALB
  TargetResponseTime: time from ALB sending to target → first byte back
  ClientTLSNegotiationErrorCount: TLS failures on client→ALB connection (client cert mismatch, old TLS)
  TargetTLSNegotiationErrorCount: TLS failures on ALB→target connection (ALB→backend cert issue)
  HTTPCode_ELB_5XX: errors generated by the ALB itself (target unavailable, timeout)
  HTTPCode_Target_5XX: errors returned by the backend targets

  Exam trap: "Clients can't connect — HTTPS handshake failing"
    → ClientTLSNegotiationErrorCount (not TargetTLS)
  Exam trap: "ALB health checks failing" → HealthyHostCount goes to 0
  Exam trap: "502 Bad Gateway from ALB" → backend is unhealthy or returning invalid response

CloudFront Metrics:
  CacheHitRate: % of requests served from cache (want this HIGH, >80%)
  BytesDownloaded vs BytesUploaded: ingress/egress from CloudFront
  4xxErrorRate: client errors (404 = object not in S3, 403 = S3 bucket policy blocking)
  5xxErrorRate: origin errors

  Exam trap: "CloudFront 403" → check S3 bucket policy + OAI/OAC configuration
  Exam trap: "CloudFront not caching" → check Cache-Control: no-store (won't cache) or query strings in cache key
```

### Common Architecture Patterns for Request-Response

```
Pattern 1: Synchronous API with caching
  Route 53 → CloudFront → API Gateway → Lambda → ElastiCache → RDS
  Cache miss: Lambda queries RDS → stores in ElastiCache → returns
  Cache hit: Lambda returns from ElastiCache (1ms), skips RDS

  Exam: "Reduce RDS read load for frequently-accessed data" → ElastiCache in front of RDS

Pattern 2: Async processing for slow tasks
  Route 53 → ALB → ECS (API) → SQS → ECS (worker) → DynamoDB
  API returns 202 Accepted immediately with jobId
  Worker processes from queue, writes result to DynamoDB
  Client polls: GET /jobs/{jobId} → ECS reads DynamoDB → returns status

  Exam: "Video transcoding request must not timeout" → async with SQS

Pattern 3: Global web application
  Route 53 (Latency-routing) → CloudFront → S3 (static) / ALB (dynamic API)
  Static assets: CloudFront → S3 (cached at edge forever until invalidated)
  API calls: CloudFront → ALB → ECS → RDS in nearest region

  Exam: "Serve static assets globally with low latency" → CloudFront + S3

Pattern 4: Microservices with service mesh
  API Gateway → ALB → ECS + App Mesh (Envoy) → services
  App Mesh: mutual TLS between services, distributed tracing (X-Ray), retry/circuit breaker

  Exam: "mTLS between microservices on ECS" → App Mesh + ACM Private CA
```

---

## SECTION 10 — Comparison Tables

### Table 1 — Request-Response Lifecycle Phase Timing Breakdown

| Phase                            | Typical Latency  | Cache Miss | Cache Hit              | Optimization Lever                                        |
| -------------------------------- | ---------------- | ---------- | ---------------------- | --------------------------------------------------------- |
| DNS Resolution                   | 0–100ms          | 50–100ms   | 0–2ms (OS cache)       | dns-prefetch, higher DNS TTL, resolve external deps early |
| TCP Handshake                    | 1×RTT (20–100ms) | 1×RTT      | 0 (Keep-Alive reuse)   | HTTP/2 (one connection), HTTP/3 (0-RTT resume)            |
| TLS Handshake                    | 1×RTT TLS 1.3    | 1×RTT      | 0 (session resumption) | TLS 1.3, session tickets, 0-RTT for repeat clients        |
| HTTP Request Transit             | ½×RTT            | ½×RTT      | ½×RTT (CDN edge)       | CDN edge PoP (reduce geographic distance)                 |
| Server Processing                | 1–500ms+         | DB queries | Served from Redis 1ms  | Caching, DB indexing, async I/O, connection pool          |
| Response Transit                 | ½×RTT            | ½×RTT      | ½×RTT                  | CDN, compress response (gzip/br)                          |
| Browser Rendering                | 20–500ms         | N/A        | N/A                    | Critical CSS inline, defer scripts, image sizing          |
| **Total (uncached)**             | **~300–1000ms**  |            |                        |                                                           |
| **Total (fully cached CDN hit)** | **~5–20ms**      |            |                        |                                                           |

---

### Table 2 — HTTP/1.1 vs HTTP/2 vs HTTP/3 Request Lifecycle

| Feature                              | HTTP/1.1                                     | HTTP/2                         | HTTP/3 (QUIC)                                |
| ------------------------------------ | -------------------------------------------- | ------------------------------ | -------------------------------------------- |
| Transport                            | TCP                                          | TCP                            | UDP (QUIC)                                   |
| Connections                          | 6-8 per domain (browser)                     | 1 per domain                   | 1 per domain                                 |
| Multiplexing                         | No (serial per conn.)                        | Yes (streams)                  | Yes (streams)                                |
| HOL Blocking                         | Per-connection                               | TCP-level (stream-independent) | None (QUIC per-stream)                       |
| Header Compression                   | None                                         | HPACK                          | QPACK                                        |
| Server Push                          | No                                           | Yes                            | Yes                                          |
| Connection Establishment             | 1 RTT TCP + TLS                              | 1 RTT TCP + TLS                | 0 RTT (QUIC resume)                          |
| Mobile/Lossy Network                 | Suffers (TCP retransmit = all streams stall) | Suffers same issue             | Better (independent stream recovery)         |
| Typical Performance Gain vs HTTP/1.1 | Baseline                                     | 10-30%                         | 10-20% on top of HTTP/2                      |
| AWS Support                          | All                                          | ALB, CloudFront                | CloudFront (HTTP/3 enabled per distribution) |

---

### Table 3 — Caching Layer Comparison in Request Lifecycle

| Cache Layer               | Location           | Latency  | Scope                            | Invalidation                       |
| ------------------------- | ------------------ | -------- | -------------------------------- | ---------------------------------- |
| Browser cache             | Client browser     | 0ms      | That user only                   | Headers (max-age, must-revalidate) |
| Service Worker cache      | Client browser     | 0ms      | Offline capable                  | SW script controls it              |
| CDN Edge cache            | Nearest PoP        | 1–5ms    | All users at that PoP            | CloudFront InvalidationAPI, Purge  |
| CDN Regional cache        | Regional PoP       | 5–20ms   | All users globally               | Same as above                      |
| API Gateway cache         | AWS managed        | 1–5ms    | Per API stage                    | Flush console/CLI, TTL             |
| Application cache (Redis) | Same region        | 0.3–1ms  | All app instances                | TTL, explicit delete, LRU eviction |
| DB query cache            | DB server RAM      | 0.01ms   | DB level (deprecated in MySQL 8) | Writes invalidate                  |
| Origin / DB               | Application server | 10–100ms | Source of truth                  | N/A                                |

---

### Table 4 — AWS Load Balancer Comparison for Request Lifecycle

| Feature                     | ALB                        | NLB                   | CLB (legacy)      |
| --------------------------- | -------------------------- | --------------------- | ----------------- |
| OSI Layer                   | Layer 7 (HTTP)             | Layer 4 (TCP/UDP)     | Layer 4/7 (basic) |
| Static IP                   | No (DNS only)              | Yes (Elastic IP)      | No                |
| TLS Termination             | Yes (ACM cert)             | Yes (ACM cert)        | Yes               |
| mTLS Support                | Yes (Trust Store, 2023+)   | No (passthrough only) | No                |
| WebSocket                   | Yes (native support)       | Yes (TCP passthrough) | Partial           |
| HTTP/2                      | Yes (client-to-ALB)        | No (TCP)              | No                |
| gRPC                        | Yes                        | No                    | No                |
| Routing                     | Path, host, query, header  | Port only             | Port only         |
| Preserve Client IP          | X-Forwarded-For header     | Native (actual IP)    | X-Forwarded-For   |
| Connections drained?        | Yes (deregistration delay) | Yes                   | Yes               |
| Health checks               | HTTP/HTTPS path            | TCP / HTTP            | TCP only          |
| Use for static IP whitelist | ❌                         | ✅                    | ❌                |
| Use for on-premises TCP     | ❌                         | ✅                    | ❌                |

---

### Table 5 — Synchronous vs Asynchronous Request-Response Patterns

| Pattern            | Example                      | Client Waits?             | Timeout Risk                | AWS Implementation           | Use When                                 |
| ------------------ | ---------------------------- | ------------------------- | --------------------------- | ---------------------------- | ---------------------------------------- |
| Sync REST          | GET /users/123               | Yes                       | API GW 29s hard limit       | API GW + Lambda              | Result needed immediately, fast response |
| Sync long-poll     | GET /jobs/123 (polling)      | Yes, short intervals      | Client retries with timeout | API GW + DynamoDB            | Check status of async job                |
| Async SQS          | POST /orders → 202           | No                        | None                        | API GW → SQS → Lambda        | Processing takes >29s                    |
| Webhook push       | POST /orders → 202, callback | No (server pushes result) | Callback delivery retries   | SQS → Lambda → SNS → HTTPS   | Client can receive callbacks             |
| Server-Sent Events | GET /feed/live               | No (stream)               | Connection keepalive        | ALB + ECS SSE endpoint       | Real-time one-directional updates        |
| WebSocket          | ws://live-auction            | No (bidirectional)        | Idle timeout 10min ALB      | API GW WebSocket API         | Chat, live bidding, collaboration        |
| Streaming response | GET /report.csv (chunked)    | Yes (stream receives)     | Per-chunk timeout           | ALB + ECS + chunked transfer | Large responses, avoid buffer overhead   |

---

## SECTION 11 — Quick Revision

### 10 Key Points

1. **Request lifecycle has 8 phases:** DNS → TCP → TLS → HTTP Request → Network Transit → Server Processing → HTTP Response → Browser Rendering. Each phase has a different fix when it's the bottleneck.

2. **TTFB captures phases 4-6 only:** DNS, TCP, and TLS happen before the HTTP request is sent; browser rendering happens after the response is received. TTFB = pure server-side + network speed indicator.

3. **Connection reuse eliminates TCP+TLS overhead:** HTTP/2 keep-alive on a single connection serves unlimited requests. Without it: each request pays 1-2 RTTs setup fee.

4. **CDN converts 200ms round trips to 5ms:** By serving from an edge PoP 5ms away (instead of origin 200ms away), CDN reduces lifecycle phases 2, 5, 6, 7 combined.

5. **N+1 query pattern is the #1 server processing killer:** One query → N subsequent queries for related data. Fix: JOIN, batch fetch, or DataLoader-style batching. Always check DB query count, not just query time.

6. **DNS TTL tradeoffs directly impact lifecycle:** Short TTL (60s): fast failover, but more DNS lookup latency for users. Long TTL (3600s): fewer lookups (faster), but slow failover. CDN / Route 53 health check failover works best with low TTL.

7. **API Gateway hard timeout is 29 seconds:** If backend processing takes longer, return 202 immediately + use async pattern (SQS + polling or webhook). No workaround for the 29s limit.

8. **Distributed tracing requires propagating W3C `traceparent` header:** Every service must read the incoming traceparent, create a child span, and forward to outbound calls. Breaking this chain makes traces incomplete.

9. **ALB deregistration delay prevents request drops during deploys:** Default 300s. For fast APIs, reduce to 30s. During blue/green: new traffic goes to green; ALB drains blue before killing it.

10. **Browser critical path: render-blocking JS/CSS increases FCP significantly:** CSS blocks rendering; sync JS blocks both parsing AND rendering. Inline critical CSS + defer/async non-critical JS + preload key fonts = fastest perceived performance.

---

### 30-Second Concept Explanation

> "The request-response lifecycle is everything that happens between a user clicking a link and seeing the page. It starts with DNS — finding the server's IP address, like looking up a phone number. Then TCP — establishing a connection, like dialing the phone. Then TLS — a security handshake, like proving your identity. Then the HTTP request travels over the network to the server. The server does its work — database queries, business logic — then sends back the response. The browser receives it and renders the visual page. Total time: 150-300ms for a well-optimized site, 3+ seconds for a poorly optimized one. Each phase has a different optimization: DNS → CDN/pre-fetch; TCP → keep-alive/HTTP2; TLS → TLS 1.3/session resumption; server → caching/DB optimization; render → critical CSS/deferred JS."

---

### Mnemonics

**Phases: "DTTHN SRR"** (Dream The Tiny Highway → Network Sends Really Right)

```
D = DNS Resolution         (0-100ms: find the IP)
T = TCP Handshake          (1 RTT: establish connection)
T = TLS Handshake          (1 RTT: secure connection)
H = HTTP Request           (structure: method, headers, body)
N = Network Transit        (physical distance to server)
S = Server Processing      (your code + DB queries)
R = HTTP Response          (status + headers + body)
R = Rendering              (browser builds the visual page)
```

**TTFB: "Time To First Byte = My Server Speed"**

- T = TCP already connected
- T = TLS already done
- F = From request sent
- B = to first Byte received
- = ONLY measures: network transit + server processing + first byte travel back

**CDN benefit: "Close, Cached, Cheap"**

- Close: edge PoP near user → short transit
- Cached: frequent content served without hitting origin
- Cheap: saves origin bandwidth and processing costs

**HTTP/2 multiplexing: "One Pipe, Many Streams"**

- One TCP connection for ALL requests from a domain
- Each request is a stream: independent, no waiting for others
- Server Push: server can PUSH resources client didn't ask for yet

**N+1 problem: "One query leads to N more = database killer"**

- Trigger: load 1 list → for each item, another query = 1+N queries
- Fix: JOIN (1 query), batch fetch, GraphQL dataloader, ORM eager loading

---

## SECTION 12 — Architect Thinking Exercise

### Problem Statement

You're the Lead Architect for a global e-commerce company. Your site currently loads in 4.2 seconds globally. The CEO has set a target: **< 1 second globally for the next Black Friday** (3 months away). Your marketing team says **"If we miss the 1s target, we lose an estimated $12M in cart abandonment."**

Current architecture:

- **DNS:** GoDaddy DNS, TTL = 86400 seconds (24 hours)
- **Hosting:** Single EC2 instance (t3.medium) in us-east-1
- **Database:** MySQL on the same EC2 instance
- **No TLS offloading** (Nginx handles TLS on EC2)
- **No caching** — every request hits MySQL
- **HTML:** 4MB of inline JavaScript (not minified)
- **Images:** 800KB product images (uncompressed TIFF files)

Measurement breakdown (Chrome DevTools, user in Munich):

- DNS: **1,100ms** (cold, no cache)
- TCP: **180ms** (NYC round trip)
- TLS: **200ms** (TLS 1.2, 2 RTT)
- TTFB: **1,900ms** (MySQL query on every page load, no connection pool)
- Content Download: **800ms** (4MB JS + uncompressed images)
- Rendering: **~200ms**
- **Total: ~4,200ms**

**Question: Design a complete architecture to achieve < 1 second globally. What do you change, in what order, and what is the expected impact of each change?**

---

_(Try to solve this yourself for 5 minutes before reading the solution below)_

---

### Solution

**Priority order: biggest impact first, fastest to implement first.**

---

**Step 1 — Asset Optimization (0 cost, 1 day, knocks off 700ms)**

```
JS bundle:
  BEFORE: 4MB inline, no minification
  AFTER: code splitting + minification + tree shaking = 200KB per page
  Tool: Webpack/Vite with production mode
  Impact: Content download drops from 800ms → 80ms (90% reduction)

Images:
  BEFORE: 800KB TIFF
  AFTER: WebP/AVIF format, lazy loading, proper dimensions
  Tool: Sharp (Node.js image resizing), <img loading="lazy">
  Impact: Image bytes drop 90%, lazy load = below-fold images don't block FCP

Critical CSS inline:
  BEFORE: render-blocking external CSS
  AFTER: 5KB critical CSS inline, rest deferred
  Impact: FCP no longer blocked by CSS file download

New content download: ~80ms (was 800ms)
New rendering: ~100ms (was ~200ms, fewer render blockers)
```

**Step 2 — CloudFront CDN (< $5/day, 2 hours, knocks off 1,500ms for European users)**

```
DNS: Move from GoDaddy to Route 53 (TTL = 60 seconds)
  DNS impact: 1100ms → 2ms (TTL cache hit after first visit)
  For new users: 50ms (Route 53 is faster than GoDaddy)

CloudFront distribution:
  Origin: EC2 in us-east-1
  Price class: All edge locations (covers Munich)

  Static assets (JS, CSS, images):
    Cache policy: CachingOptimized (max-age=31536000, immutable with hashed filenames)
    Munich → Frankfurt PoP: ~2ms
    Content download: 80ms → 5ms (served from Frankfurt edge)

  HTML (dynamic pages):
    Cache-Control: no-store (user-specific) OR max-age=60 (public pages)
    Even with no cache: Munich → Frankfurt (CloudFront edge) → us-east-1 via AWS backbone
    Munich transit: 2ms to Frankfurt PoP vs 90ms direct to NYC

  TLS: ACM cert (TLS 1.3 supported by CloudFront automatically)
    TLS 1.2 2 RTT (200ms) → TLS 1.3 1 RTT to nearest PoP (~10ms)

After CDN:
  DNS: 2ms (cached), 50ms (cold)
  TCP: 5ms (to Frankfurt PoP, not NYC)
  TLS: 5ms (to Frankfurt PoP, TLS 1.3)
  TTFB: still 1900ms ← THIS IS NOW THE BOTTLENECK
  Content: 5ms (edge cached)
  Render: 100ms
  Total for repeat visitors: 5+5+5+1900+5+100 = ~2,020ms (better, but TTFB dominates!)
```

**Step 3 — Application Layer: Eliminate High TTFB (1 week, knocks off 1,500ms)**

```
Decompose the 1,900ms TTFB:
  Connection setup to MySQL: no pool = new TCP/auth per request = ~30ms
  MySQL query: SELECT * FROM products (full table scan, 50,000 rows) = ~800ms
  Business logic: PHP without opcode cache = ~200ms
  JSON serialization of 50,000 products = ~500ms
  Gzip compression: ~200ms (no async streaming, blocking)

Fix 1: Database connection pool (HikariCP / PgBouncer)
  30ms connection overhead → 0ms (borrowed from pool)

Fix 2: Paginate the products query
  BEFORE: SELECT * FROM products (50,000 rows)
  AFTER: SELECT * FROM products WHERE category_id=5 LIMIT 20 OFFSET 0
  Add index on category_id (CREATE INDEX idx_products_category ON products(category_id))
  Query time: 800ms → 5ms

Fix 3: ElastiCache Redis for product catalog
  Product catalog doesn't change every second
  Cache with 5-minute TTL: Redis GET products:category:5 → 0.5ms
  On cache miss: DB query (5ms) → store in Redis → return

Fix 4: Separate EC2 into ALB + ECS + RDS (not on same machine!)
  RDS Multi-AZ: dedicated database server, faster I/O
  ECS: auto-scaling for Black Friday spike (can scale to 50 containers instantly)

New TTFB breakdown:
  Redis cache hit: 0.5ms
  Redis cache miss: 5ms (DB query) + 1ms (cache write) = 6ms
  JSON serialize 20 items: 1ms
  Gzip: 1ms
  New TTFB: ~10ms (cache hit) / ~15ms (miss)
```

**Step 4 — HTTP/2 and CloudFront Configuration**

```
Enable HTTP/2 on CloudFront distribution (check: ✓ HTTP/2 — on by default since 2016)
  Browser now fetches JS, CSS, images over 1 connection = 60-80% fewer connection setup delays

Enable HTTP/3 on CloudFront:
  Munich mobile users: ~20% latency improvement due to QUIC

CloudFront cache for API responses:
  GET /products?category=electronics: cache at CloudFront edge for 60 seconds
  100,000 users/minute, same category → 99.9% cache hit ratio
  Origin receives: 100 req/min instead of 100,000 req/min
```

**Final Architecture:**

```
Route 53 (TTL=60)
  → CloudFront (edge PoP: Frankfurt for Munich users)
      → S3 (static: JS/CSS/images, immutable cache)
      → ALB (dynamic: HTML pages and API)
          → ECS (application containers, auto-scaling 2→50 on Black Friday)
              → ElastiCache Redis (product catalog cache, session data)
              → RDS Aurora MySQL (Multi-AZ, read replicas for read scaling)
              → SQS (async: order processing, email sending)
```

**Final timing for Munich user (repeat visit):**

| Phase            | Before       | After                                   |
| ---------------- | ------------ | --------------------------------------- |
| DNS              | 1,100ms      | 2ms (cached)                            |
| TCP              | 180ms        | 5ms (to Frankfurt)                      |
| TLS              | 200ms        | 5ms (TLS 1.3 to Frankfurt)              |
| TTFB             | 1,900ms      | 10ms (Redis cache hit)                  |
| Content Download | 800ms        | 5ms (edge cached assets)                |
| Rendering        | ~200ms       | 80ms (deferred JS, inline critical CSS) |
| **Total**        | **~4,200ms** | **~107ms**                              |

**Target achieved: < 1 second globally. ✓**

**Key lessons:**

1. Measure before optimizing — "optimize what you can prove is slow"
2. Asset optimization and CDN together eliminate 3,000ms with < 1 day of work
3. TTFB reveals the server-side story — always investigate the slowest sub-phase first
4. Caching at the right layer (Redis for data, CloudFront for responses) compounds savings
5. Architecture separation (EC2 monolith → ECS+RDS+ElastiCache) enables scaling for Black Friday spike, which pure optimization cannot address

---

## Quick Reference Card

```
curl timing command:
  curl -o /dev/null -s -w "\nDNS: %{time_namelookup}s\nTCP: %{time_connect}s\nTLS: %{time_appconnect}s\nTTFB: %{time_starttransfer}s\nTotal: %{time_total}s\n" https://your.api.com/endpoint

AWS X-Ray Python:
  from aws_xray_sdk.core import xray_recorder
  @xray_recorder.capture('getUser')
  def get_user(user_id): ...

Key AWS timeouts:
  API Gateway: 29s (hard limit)
  Lambda: 15 min (max)
  ALB idle: 60s default, 4000s max
  ALB deregistration: 300s default, 0-3600s
  CloudFront default TTL: 86400s (1 day)
  CloudFront min TTL: 0s
```
