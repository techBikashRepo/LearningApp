# File Upload Flow — Part 3 of 3

### Sections: 9 (Interview Prep), 10 (Comparison Tables), 11 (Quick Revision), 12 (Architect Thinking Exercise)

**Series:** Backend & API Design → REST APIs → System Design

---

## SECTION 9 — Interview Questions & Patterns

### Beginner-Level Questions

**Q: Why shouldn't you upload files directly through your API server?**

```
Three concrete reasons:

1. Memory limits
   Lambda max memory: 10GB. But most are configured at 128–512MB.
   A 50MB file upload fully loaded into Lambda memory = crashes at low memory setting.
   100 concurrent 50MB uploads = 5GB of server memory consumed by file transfers.
   API servers are optimized for fast JSON processing, not binary file streaming.

2. Timeout limits
   API Gateway: 29-second hard timeout.
   A slow mobile upload of 200MB at 2 Mbps = 800 seconds.
   Direct Lambda upload: request times out, user loses all progress.

3. Bandwidth and cost
   Every byte passing through your API server = compute cost + network transfer cost.
   S3 direct upload: client → S3 (zero API server bandwidth usage).
   S3 is architected for massive concurrent binary transfers (terabytes/second globally).

Correct pattern: presigned URL — API server signs a temporary upload URL,
client uploads directly to S3, API server never handles raw file bytes.
```

---

**Q: What is a presigned URL and how does it work?**

```
A presigned URL is a pre-authorized S3 URL generated by your server that lets a client
upload (or download) a specific file directly to/from S3 without needing AWS credentials.

How it works:
1. Your server uses its AWS credentials to generate a signed URL:
   - Specifies: bucket, key (path), Content-Type, ContentLength, expiry
   - Signs using HMAC-SHA256 with AWS credentials
   - Produces: https://bucket.s3.amazonaws.com/key?X-Amz-Signature=...

2. Client receives this URL and uses it for a direct PUT:
   PUT https://bucket.s3.amazonaws.com/uploads/user123/file.pdf?X-Amz-Signature=abc...
   Content-Type: application/pdf
   [binary data]

3. S3 verifies the signature against the request parameters:
   - If parameters match the signature: 200 OK, file stored
   - If Content-Type changed: 403 Forbidden (signature mismatch)
   - If URL expired: 403 Forbidden (past X-Amz-Expires)

Security properties:
  - Time-limited: expires after configured time (typically 15 minutes)
  - Path-limited: can only upload to the one specific S3 key
  - Type-limited: Content-Type is signed, cannot be changed
  - No AWS credentials needed client-side: client cannot access other S3 resources
```

---

### Intermediate-Level Questions

**Q: How do you handle file type validation for uploads?**

```
Three-layer approach (defense in depth):

Layer 1 — Before upload (client/API side, fast feedback):
  At initiate step, check content_type matches allowed list for the purpose.
  Reject video/mp4 claiming to be an invoice early — no S3 credit wasted.
  BUT: client provides Content-Type, so this is user-controlled. Insufficient alone.

Layer 2 — Magic bytes (after S3 upload, Lambda scan):
  Every file format starts with a known byte sequence.
  PDF starts with %PDF (25 50 44 46).
  JPEG starts with FF D8 FF.
  A user who renames malware.exe to invoice.pdf still has MZ header (4D 5A).
  Lambda reads first 8 bytes off S3 stream: no match → reject before storing.

  Key: you are reading what the file IS, not what the user claims it is.

Layer 3 — ClamAV scan (after magic bytes pass):
  Magic bytes confirm file type. But a valid PDF can contain embedded malware.
  ClamAV checks against virus signature database.
  Catches known malware embedded in valid file containers.

  Tool: ClamScan Lambda layer or VirusTotal API (external service).

Bonus layer — re-encoding:
  For images specifically: Sharp (Node.js) or Pillow (Python) decode and re-encode.
  Re-encoding strips all metadata, embedded scripts, and polyglot payloads.
  The output image is structurally clean — only valid image pixels survive.
  PDF equivalent: PDF normalization tools parse and re-render to a clean PDF tree.
```

---

**Q: A user uploads a 500MB video file. The upload takes 8 minutes and fails at 95%. What do you do?**

```
This is the multipart upload problem.

Without multipart:
  8 minutes * 0.95 = 7.6 minutes of transfer lost. User must restart from byte 0.
  On mobile with an unreliable connection: upload may never complete.

With S3 multipart upload + resume:

Design:
  Split 500MB into 50 parts (10MB each, minimum S3 part size is 5MB)
  Upload each part individually with its own retry logic
  Completed parts are persisted by S3 under the UploadId

  On failure at 95% (part 48 or 49 fails):
  1. Client receives error response from S3 for that part
  2. Client calls your API: GET /uploads/{upload_id} → { parts_uploaded: [1..47], upload_id }
  3. Or: client calls S3 ListParts API (if given permission) → list of completed parts
  4. Client retries only the failed part(s)
  5. Once all 50 parts complete: CompleteMultipartUpload request → file assembled by S3

What this requires:
  1. Client stores UploadId (S3 multipart session) and which parts succeeded locally
  2. Server records UploadId in DynamoDB linked to your upload_id
  3. If client closes browser and returns: server provides UploadId → resume
  4. Async parallel uploads: 5 parts simultaneously → 5x faster

Part size strategy for 500MB:
  50 parts × 10MB = 500MB
  Allow 5 parallel → 10 batches × (estimated 3-5 seconds/batch) = 30-50 seconds on fast connection
  vs 8 minutes sequential (one part at a time)
```

---

### Advanced Question

**Q: Design a secure file download system where only authorized users can access uploaded files.**

```
Problem: Files are in S3. S3 public access is blocked. How do users download them?

OPTION 1 — Your API proxies the download (BAD):
  GET /documents/{id}/download
  Lambda fetches from S3 → streams to client

  Problems:
  - Lambda bandwidth cost (you pay per byte sent through Lambda)
  - Lambda memory: 100MB file → 100MB Lambda memory
  - Lambda timeout: large files may exceed timeout
  - One Lambda processing one download = not scalable

OPTION 2 — S3 direct presigned GET URL (BETTER):
  GET /documents/{id}/download
  Lambda: check auth → generate presigned GET URL → redirect 302

  const downloadUrl = await getSignedUrl(s3, new GetObjectCommand({
    Bucket: 'invoiceflow-prod',
    Key: document.s3_key,
    ResponseContentDisposition: `attachment; filename="${document.filename}"`,
    ResponseContentType: document.content_type,
  }), { expiresIn: 300 });  // 5-minute download window

  return { statusCode: 302, headers: { Location: downloadUrl } };

  Problems:
  - S3 URL exposed to client (exposes bucket name, object key structure)
  - S3 URL is not CDN-cached → high S3 GET request costs at scale
  - S3 URL shows exact S3 key → information disclosure

OPTION 3 — CloudFront Signed URLs (PRODUCTION STANDARD):
  S3 bucket: not publicly accessible
  CloudFront: sits in front of S3, attached Key Group for signed URLs

  Your Lambda generates a CloudFront signed URL:
  const signer = new CloudFrontClient.getSignedUrl({
    url: `https://cdn.invoiceflow.com/${document.s3_key}`,
    keyPairId: process.env.CF_KEY_PAIR_ID,
    privateKey: process.env.CF_PRIVATE_KEY,
    dateLessThan: new Date(Date.now() + 300000).toISOString(),
    // Optional: restrict to specific IP range for high-security docs
  });

  return { statusCode: 302, headers: { Location: signedUrl } };

  Benefits:
  - Files cached at CloudFront edge (cheap, fast, globally distributed)
  - URL points to your CDN domain — bucket structure hidden
  - Signature tied to URL + expiry + optionally IP address
  - If signature is missing or expired → CloudFront returns 403 (never reaches S3)
  - Works for large files: CloudFront streams from S3, resumable range requests

SECURITY MATRIX:

  Before generating any download URL:
  1. Verify JWT is valid (Cognito authorizer)
  2. Verify document ownership: document.user_id === jwt.sub
     OR: document is shared with this user (separate share table lookup)
  3. Verify document status === 'ready' (not processing, not rejected)
  4. Log download event: user_id, document_id, timestamp, IP (audit trail)

  Response codes:
  Owned document, status ready → 302 to CloudFront signed URL
  Document not found → 404
  Wrong owner → 403 INSUFFICIENT_PERMISSIONS
  Document not ready → 409 DOCUMENT_NOT_READY { status: 'processing' }
  Document rejected → 410 Gone { status: 'rejected', reason: 'VIRUS_DETECTED' }
```

---

## SECTION 10 — Comparison Tables

### Upload Method Comparison

```
METHOD                    PROS                             CONS
──────────────────────────────────────────────────────────────────────────────────
Direct to API Server      Simple to implement              Memory/timeout constraints
                          Full control of file path        Bandwidth cost on API tier
                          Easy access control              Not scalable for large files
  Use for: small files < 1MB only

Presigned PUT URL         No server bandwidth used         Two-step flow for client
(single-part)             Scalable, S3 handles load        Files < 5GB only (S3 limit)
                          Works within Lambda limits        Presigned URL expires
  Use for: most uploads up to a few hundred MB

S3 Multipart Upload       Resume from failure              Complex client implementation
                          Parallel uploads (faster)        Requires UploadId management
                          No file size limit (5TB max)     Incomplete parts cost money
  Use for: files > 100MB or unreliable connections

Server-side proxy         Virus scan before storage        Very slow, memory-intensive
(download via Lambda)     Control exact byte inspection    Only viable for small files
  Use for: strict compliance where file must not touch S3 before validation only
```

### File Security Validation Layers

```
LAYER                     WHAT IT CATCHES               WHEN IT RUNS
──────────────────────────────────────────────────────────────────────────────────
Content-Type header check  Wrong file type claimed        Before upload (MS)
                           (user-controlled — weak)

File size check            Oversized files                Before upload (MS)

Magic bytes check          File content ≠ claimed type    After upload (Lambda)
                           Forbids EXE/ELF/script files

ClamAV signature scan      Known malware signatures       After upload (Lambda)
                           Virus-infected valid files

Image re-encoding          Polyglot attacks               After scan (Lambda)
                           Embedded JS in images          Re-encode with Sharp/Pillow
                           EXIF-based exploits

PDF normalization           PDF exploits                   After scan (Lambda, optional)
                           JavaScript in PDFs
```

### Download Security Options

```
METHOD                SECURITY       CDN CACHED?  COST          BEST FOR
──────────────────────────────────────────────────────────────────────────────────
Lambda proxy           High           No           High (egress  Small private files
                                                   + Lambda)     compliance requirements

S3 Presigned GET URL   Medium         No           Low (S3 only) Internal/dev use
                                                                  Bucket structure exposed

CloudFront Signed URL  High           Yes          Lowest        Production downloads
                       (+ IP lock option)          (CFN pricing) Public-facing APIs

CloudFront + WAF       Highest        Yes          Medium        High-security (banking)
  + OAC (no S3 public) (blocks bots,              (WAF cost)    HIPAA, PCI environments
                        rate limits)
```

---

## SECTION 11 — Quick Revision

### 10 Core Takeaways

```
1. Never upload through your API server for anything larger than a small thumbnail.
   Memory limits, timeout limits, bandwidth cost — all three break at scale.
   Presigned URL = client uploads directly to S3.

2. Three-step upload flow: initiate → direct S3 upload → confirm.
   initiate: server validates, stores intent, generates presigned URL
   S3 upload: client to S3, your server sees nothing
   confirm: client tells your API the upload is done, creates DB record

3. Content-Type validation alone is insufficient — validate magic bytes.
   User provides Content-Type. Attacker renames malware.exe to invoice.pdf.
   Magic bytes read what the file IS, not what the user claims.

4. Files > 100MB should always use S3 multipart upload.
   Enables parallel part uploads (5x–10x speed improvement).
   Enables resume on network failure (no restart from byte 0).
   Minimum part size: 5MB. Maximum single PUT: 5GB.

5. Three-bucket architecture: staging → scan → production.
   Untrusted files never reach the production bucket.
   Infected files quarantined separately for forensics.
   Staging bucket has short lifecycle (auto-delete unconfirmed uploads).

6. Presigned URL should have short expiry — 10-15 minutes.
   Long expiry = window for URL to leak and be misused.
   If expired: re-issue a new presigned URL (same multipart UploadId).

7. Never trust the filename from the client — generate your own S3 key.
   Path traversal: filename = "../../etc/passwd" → your key becomes dangerous.
   Generate: uploads/{userId}/{uuid}.{sanitized_ext}

8. Download security: CloudFront signed URLs, not S3 presigned GET.
   S3 presigned URL exposes bucket name and key structure.
   CloudFront signed URL hides S3 details, uses your domain, is CDN-cached.

9. Always set Content-Disposition: attachment for download URLs.
   Prevents browser from rendering and potentially executing file content.
   Especially critical for HTML, SVG, and polyglot files.

10. Set a lifecycle rule to abort incomplete multipart uploads after 1–7 days.
    Incomplete parts are billed as stored data.
    Without lifecycle rule: GB of orphaned parts accumulate and cost money.
```

### 30-Second Explanation

> "The file upload pattern works in three steps: first, the client asks the API server for a presigned upload URL — the server validates the request, stores the upload record, and returns a temporary S3 URL. Second, the client uploads the file directly to S3 using that URL — the API server never handles the file bytes. Third, the client calls the API to confirm the upload, which triggers a scan Lambda that validates the file's magic bytes, runs a virus scan, and only after passing both checks moves the file from the staging bucket to the production bucket. For large files you layer S3 multipart on top, which allows resume on failure and parallel part uploads."

### Mnemonics

```
UPLOAD FLOW — "ISC" (Initiate, Send, Confirm):
  I — Initiate: server validates + generates presigned URL
  S — Send: client sends file directly to S3
  C — Confirm: client tells API it's done → triggers scan pipeline

FILE VALIDATION — "MAGIC":
  M — Magic bytes (read file content, not claimed type)
  A — Antivirus scan (ClamAV signatures)
  G — Generate clean key (never trust client filename)
  I — Image re-encoding (destroy polyglot payloads)
  C — Content-Type pre-check (fast fail, user-controlled)

THREE BUCKETS — "SPQ":
  S — Staging (untrusted uploads land here)
  P — Production (clean, scanned files only)
  Q — Quarantine (failed scans, forensics)

PRESIGNED URL RULE — "TELL":
  T — Time-limited (short expiry, 15 minutes)
  E — Exact path only (one S3 key)
  L — Locked to Content-Type (signed into URL)
  L — Locked to Content-Length (signed into URL)
```

---

## SECTION 12 — Architect Thinking Exercise

### Exercise: MediScan — Medical Image Upload System

**Scenario:**

You are the backend architect for MediScan, a healthcare platform where radiologists upload diagnostic images (X-rays, MRI scans, CT scans) that are stored, processed for AI-assisted diagnosis, and retrieved by physicians at hospitals worldwide.

**System requirements:**

- Images are DICOM format (medical imaging standard, typically 20MB–2GB per study)
- Regulatory: HIPAA compliant (PHI in file metadata — patient name, DOB, MRN)
- Radiologists at hospitals upload from low-bandwidth hospital networks (often < 5 Mbps)
- AI diagnosis pipeline must run on every image before a physician can view it
- Physicians at remote clinics need fast access (global audience)
- Retention: images must be stored for 7 years minimum (legal requirement)
- Audit: every upload and every download must be logged (HIPAA audit trail)

**Design challenge:** Design the complete upload, storage, and download architecture.

---

### Model Solution

**Upload Architecture**

```
CHALLENGE: DICOM files are 20MB–2GB, uploaded on hospital slow networks

1. Always multipart upload (5MB parts):
   Client: MediScan desktop app (not browser — need desktop for DICOM viewing)

   Initiation:
   POST /studies/upload-session
   { patient_mrn, study_date, modality: 'MRI', file_count, total_size_bytes }

   Server:
   - Validates that MRN belongs to a patient in this hospital's tenant (authorization)
   - Creates study record in DynamoDB (status: pending)
   - Returns: session_id, presigned multipart upload parameters per file

   Desktop app:
   - Splits each DICOM file into 5MB parts
   - Uploads 3 parts concurrently (bandwidth-aware: don't saturate hospital network)
   - On part failure: retry 3 times with exponential backoff
   - Resumes automatically after network interruption

2. HIPAA DICOM metadata handling:
   CHALLENGE: DICOM headers contain PHI (patient name, DOB, MRN, referring physician)
   HIPAA requires PHI to be stored encrypted and access-logged

   Approach:
   - Client strips PHI from DICOM header before upload (DICOM de-identification)
   - Or: Lambda strips PHI from DICOM after upload, before storing to production
   - PHI stored separately in encrypted DynamoDB (not in the file itself)
   - S3 key never contains patient-identifiable info: studies/{study_id}/{file_uuid}.dcm

3. Staging / Scan / Production:
   CHALLENGE: infected DICOM files could crash diagnostic AI (known attack vector)

   staging-bucket → ClamAV Lambda → production-bucket
   On scan fail: quarantine + alert security team + notify radiologist with
   "Study upload failed security validation — contact IT"
   Patient-safe: never expose reason (patient may not know radiologist uploaded to cloud)
```

**Storage Architecture**

```
PHI ENCRYPTION STRATEGY:
  DICOM files: S3 SSE-KMS with customer-managed KMS key
  PHI DynamoDB table: encrypted with separate KMS key
  KMS key policies: only specific Lambda execution roles can decrypt
  KMS key rotation: automatic annual rotation

RETENTION STRATEGY:
  Immediate: S3 Standard (fast retrieval for recently uploaded, AI processing)
  After 90 days: S3 Standard-IA (infrequent access, 40% cost savings)
  After 1 year: S3 Glacier Instant Retrieval (rare access, 68% cost savings)
  After 7 years: S3 Glacier Deep Archive + legal hold tag (freeze, no delete)

  CRITICAL: S3 Object Lock for studies tagged as potential litigation
    aws s3api put-object-legal-hold --bucket mediscan-prod
      --key studies/st_abc/dcm_xyz.dcm
      --legal-hold Status=ON
  Object Lock WORM (write-once, read-many): not even AWS root can delete.

AUDIT LOGGING (HIPAA requirement — all access must be logged):
  S3 Server Access Logging: every GET, PUT, DELETE → audit-logs bucket
  Log format: timestamp, requester_identity, operation, object_key, status

  Application-level: every download event logged to CloudWatch Logs:
  { "event": "document_download", "user_id": "...", "study_id": "...",
    "purpose": "diagnosis",  "timestamp": "...", "ip_address": "..." }

  CloudWatch log group retained 7 years (matches DICOM retention).
```

**Download Architecture**

```
GLOBAL ACCESS CHALLENGE: radiologists in Tokyo, London, São Paulo
Without CDN: all requests hit us-east-1 S3 → 200ms+ latency for 2GB DICOM study

Solution: CloudFront with S3 Transfer Acceleration:
  Upload: S3 Transfer Acceleration routes to nearest AWS edge → faster for hospitals
  Download: CloudFront caches DICOM files at edge PoPs globally

  But: HIPAA requires knowing exactly who accessed what
  CloudFront signed URL: per-physician, per-study, time-limited

  Access control:
  GET /studies/{study_id}/access-token
  JWT must contain: physician credentials, hospital affiliation, study access permission

  Lambda checks:
  1. Physician in verified panel for this patient (or treating physician)
  2. Study access not restricted (e.g., psychiatric records may have extra guards)
  3. Study status === 'ai_complete' (AI diagnosis must complete before physician view)

  Returns CloudFront signed cookie (not URL — allows downloading multi-file studies):
  Set-Cookie: CloudFront-Policy=...; CloudFront-Signature=...; CloudFront-Key-Pair-Id=...

  Cookie restricts: all files under /studies/{study_id}/* for 4 hours
  Physician DICOM viewer app uses cookie for all subsequent file downloads.

AUDIT ON DOWNLOAD:
  CloudFront access logs → Kinesis Firehose → S3 audit bucket (HIPAA audit trail)
  Application logs: physician_id, study_id, access_time, hospital location

  Quarterly HIPAA audit: pull logs, verify all accesses had documented medical purpose.
```

**AI Pipeline Integration**

```
AI DIAGNOSIS MUST COMPLETE BEFORE PHYSICIAN CAN VIEW:

After virus scan passes and file reaches production bucket:
  S3 ObjectCreated event → AI Pipeline SQS Queue
  AI Lambda: download DICOM, run inference model (GPU Lambda or SageMaker)

  On AI complete:
  - Store diagnosis report linked to study_id
  - Update study status: 'ai_complete'
  - Send webhook to hospital system: study ready for review

  Access control enforced in download Lambda:
  if (study.status !== 'ai_complete') {
    throw new ApiError(409, 'STUDY_NOT_READY',
      'AI diagnosis is still processing. Estimated completion: 5 minutes.',
      { study_id, estimated_completion: study.ai_eta });
  }

FAILURE HANDLING:
  AI pipeline fails (model crash, invalid DICOM format):
  - Study status: 'ai_failed'
  - Physician can still view study (clinical need > AI dependency)
  - Status: 'ai_failed' → physician view enabled with warning banner
  - AI failure logged, re-queue for retry

  RULE: Never block urgent clinical access due to AI system failure.
        AI enhancement is downstream of the core clinical workflow.
```

---

_End of Topic 09: File Upload Flow_
